{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터 분석 모형은 숫자로 구성된 고정 차원 벡터를 독립 변수로 하고 있으므로 문서(document)를 분석을 하는 경우에도 숫자로 구성된 특징 벡터(feature vector)를 문서로부터 추출하는 과정이 필요하다. 이러한 과정을 문서 전처리(document preprocessing)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW (Bag of Words)\n",
    "\n",
    "문서를 숫자 벡터로 변환하는 가장 기본적인 방법은 BOW (Bag of Words) 이다. BOW 방법에서는 전체 문서 $\\{D_1, D_2, \\ldots, D_n\\}$ 를 구성하는 고정된 단어장(vocabulary) $\\{W_1, W_2, \\ldots, W_m\\}$ 를  만들고 $D_i$라는 개별 문서에 단어장에 해당하는 단어들이 포함되어 있는지를 표시하는 방법이다.\n",
    "\n",
    "$$ \\text{ 만약 단어 } W_j \\text{가 문서} D_i \\text{ 안에 있으면 }, \\;\\; \\rightarrow x_{ij} = 1 $$ \n",
    "\n",
    "## Scikit-Learn 의 문서 전처리 기능\n",
    "\n",
    "Scikit-Learn 의 feature_extraction.text 서브 패키지는 다음과 같은 문서 전처리용 클래스를 제공한다.\n",
    "\n",
    "* [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): \n",
    " * 문서 집합으로부터 단어의 수를 세어 카운트 행렬을 만든다.\n",
    "* [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html): \n",
    " * 문서 집합으로부터 단어의 수를 세고 TF-IDF 방식으로 단어의 가중치를 조정한 카운트 행렬을 만든다.\n",
    "* [`HashingVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html): \n",
    " * hashing trick 을 사용하여 빠르게 카운트 행렬을 만든다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and         0\n",
       "document    1\n",
       "first       2\n",
       "is          3\n",
       "last        4\n",
       "one         5\n",
       "second      6\n",
       "the         7\n",
       "third       8\n",
       "this        9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is the second document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 옵션\n",
    "\n",
    "`CountVectorizer`는 다양한 인수를 가진다. 그 중 중요한 것들은 다음과 같다.\n",
    "\n",
    "* `stop_words` : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    " * stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "* `analyzer` : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    " * 단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램 \n",
    "* `tokenizer` : 함수 또는 None (디폴트)\n",
    " * 토큰 생성 함수 .\n",
    "* `token_pattern` : string\n",
    " * 토큰 정의용 정규 표현식 \n",
    "* `ngram_range` : (min_n, max_n) 튜플\n",
    " * n-그램 범위 \n",
    "* `max_df` : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    " * 단어장에 포함되기 위한 최대 빈도\n",
    "* `min_df` : 정수 또는 [0.0, 1.0] 사이의 실수.  디폴트 1\n",
    " * 단어장에 포함되기 위한 최소 빈도 \n",
    "* `vocabulary` : 사전이나 리스트\n",
    " * 단어장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. 보통 영어의 관사나 접속사, 한국어의 조사 등이 여기에 해당한다. `stop_words` 인수로 조절할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'last': 2, 'one': 3, 'second': 4, 'third': 5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰(token)\n",
    "\n",
    "토큰은 문서에서 단어장을 생성할 때 하나의 단어가 되는 단위를 말한다. `analyzer`, `tokenizer`, `token_pattern` 등의 인수로 조절할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '?': 2,\n",
       " 'a': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'l': 10,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " 'o': 13,\n",
       " 'r': 14,\n",
       " 's': 15,\n",
       " 't': 16,\n",
       " 'u': 17}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'third': 1, 'this': 2}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jihoon_Kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " '?': 1,\n",
       " 'and': 2,\n",
       " 'document': 3,\n",
       " 'first': 4,\n",
       " 'is': 5,\n",
       " 'last': 6,\n",
       " 'one': 7,\n",
       " 'second': 8,\n",
       " 'the': 9,\n",
       " 'third': 10,\n",
       " 'this': 11}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-그램\n",
    "\n",
    "n-그램은 단어장 생성에 사용할 토큰의 크기를 결정한다. 1-그램은 토큰 하나만 단어로 사용하며 2-그램은 두 개의 연결된 토큰을 하나의 단어로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and the': 0,\n",
       " 'first document': 1,\n",
       " 'is the': 2,\n",
       " 'is this': 3,\n",
       " 'last document': 4,\n",
       " 'second document': 5,\n",
       " 'second second': 6,\n",
       " 'the first': 7,\n",
       " 'the last': 8,\n",
       " 'the second': 9,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'this is': 12,\n",
       " 'this the': 13}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'the third': 1, 'third': 2, 'this': 3, 'this the': 4}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도수\n",
    "\n",
    "`max_df`, `min_df` 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있다. 토큰의 빈도가 `max_df`로 지정한 값을 초과 하거나 `min_df`로 지정한 값보다 작은 경우에는 무시한다. 인수 값은 정수인 경우 횟수, 부동소수점인 경우 비중을 뜻한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'document': 0, 'first': 1, 'is': 2, 'this': 3},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법이다. \n",
    "\n",
    "\n",
    "구제적으로는 문서 $d$(document)와 단어 $t$ 에 대해 다음과 같이 계산한다.\n",
    "\n",
    "$$ \\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(t) $$\n",
    "\n",
    "\n",
    "여기에서\n",
    "\n",
    "* $\\text{tf}(d, t)$: 단어의 빈도수\n",
    "* $\\text{idf}(t)$ : inverse document frequency \n",
    " \n",
    " $$ \\text{idf}(t) = \\log \\dfrac{n_d}{1 + \\text{df}(t)} $$\n",
    " \n",
    "* $n_d$ : 전체 문서의 수\n",
    "* $\\text{df}(t)$:  단어 $t$를 가진 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.38947624,  0.55775063,  0.4629834 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941651,  0.        ,  0.4629834 ],\n",
       "       [ 0.        ,  0.24151532,  0.        ,  0.28709733,  0.        ,\n",
       "         0.        ,  0.85737594,  0.20427211,  0.        ,  0.28709733],\n",
       "       [ 0.55666851,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.55666851,  0.        ,  0.26525553,  0.55666851,  0.        ],\n",
       "       [ 0.        ,  0.38947624,  0.55775063,  0.4629834 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941651,  0.        ,  0.4629834 ],\n",
       "       [ 0.        ,  0.45333103,  0.        ,  0.        ,  0.80465933,\n",
       "         0.        ,  0.        ,  0.38342448,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Trick\n",
    "\n",
    "`CountVectorizer`는 모든 작업을 메모리 상에서 수행하므로 처리할 문서의 크기가 커지면 속도가 느려지거나 실행이 불가능해진다. 이 때  `HashingVectorizer`를 사용하면 해시 함수를 사용하여 단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 112863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time hv.transform(twenty.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석기 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bought': 0,\n",
       " 'buying': 1,\n",
       " 'buys': 2,\n",
       " 'image': 3,\n",
       " 'imagination': 4,\n",
       " 'imagine': 5,\n",
       " 'imaging': 6}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"imaging\", \"image\", \"imagination\", \"imagine\", \"buys\", \"buying\", \"bought\"]\n",
    "vect = CountVectorizer().fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "docs = twenty.data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write': 0,\n",
       " 'writer': 1,\n",
       " 'writers': 2,\n",
       " 'writes': 3,\n",
       " 'writing': 4,\n",
       " 'writing_': 5,\n",
       " 'written': 6}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\", token_pattern=\"wri\\w+\").fit(docs)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write': 0, 'writer': 1, 'writing_': 2, 'written': 3}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.s = SnowballStemmer('english')\n",
    "        self.t = CountVectorizer(stop_words=\"english\", token_pattern=\"wri\\w+\").build_tokenizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.s.stem(t) for t in self.t(doc)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer=StemTokenizer()).fit(docs)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python을 사용한 웹 서비스 개요\n",
    "\n",
    "Python은 현재 웹 서비스 부분에서 활발히 사용되고 있는 언어의 하나이다. 파이썬을 사용하여 웹서비스를 구축하는 경우, 보통 다음과 같은 구조를 가지게 된다.\n",
    "\n",
    "<img src=\"https://datascienceschool.net/upfiles/015a1deab591474a80637365668a8bd6.png\" style=\"width:100%;\">\n",
    "\n",
    "\n",
    "## 리버스 프락시 서버\n",
    "\n",
    "일반적으로 보안 문제나 정적 파일(static file)의 빠른 서비스, 로드 밸런싱 등을 위해 리버스 프락시(reverse proxy) 서버를 실제 웹 서비스 서버 앞단에 두는 경우가 많다. 리버스 프락시 서버는 클라이언트로부터의 요청을 실제 웹서버나 파일서버로 전달하거나 대리하는 역할을 한다.\n",
    "\n",
    "많이 쓰이는 리버스 프락시 서버로는 nginx 등이 있다.\n",
    "\n",
    "## WSGI 웹 어플리케이션 서버\n",
    "\n",
    "웹 어플리케이션 서버(Web Application Server)는 실제로 클라이언트 호출에 대응하여 HTML 파일 등의 결과물을 출력하는 역할을 하는 서버 프로세스이다. \n",
    "\n",
    "클라이언트 호출은 원격에서 이루어지는 함수 호출에 비유할 수 있다. 실제로 함수 호출을 할 수 있는 프로세스는 웹 어플리케이션 서버이지만 함수 자체는 웹 어플리케이션(Web Application)이라고 부르는 일종의 라이브러리와 같은 형태로 구현되어 웹 어플리케이션 서버가 해당 어플리케이션(라이브러리)를 임포트하여 사용한다.\n",
    "\n",
    "Python의 경우에는 WSGI(Web Server Gateway Interface)이라고 부르는 웹 어플리케이션 규약이 존재하여 모든 웹 어플리케이션은 WSGI 규약에 맞게 구현되어야 한다.\n",
    "\n",
    "웹 어플리케이션 서버로는 Apache에 modwsgi 모듈을 추가하여 사용할 수 있지만 Python으로 만들어진 웹 어플리케이션을 임포트하기 때문에 웹 어플리케이션 서버도 Python인 경우가 많다. tornado, uwsgi, gnuicorn, werkzeug, twisted 이 많이 사용되고 있다. \n",
    "\n",
    "* 웹 어플리케이션 서버의 성능 비교: http://nichol.as/benchmark-of-python-web-servers\n",
    "\n",
    "## WSGI 웹 어플리케이션\n",
    "WSGI: 파이썬에서 사용하는 Web application server와 web application이 통신하는 규약임.\n",
    "\n",
    "WSGI 웹 어플리케이션은 실제로 다음과 같이 environment(또는 context)와 request 인수를 받아서 response 를 출력하는 함수의 집합이다.\n",
    "\n",
    "웹 서비스의 각 URL은 이 함수들에 매핑(mapping)되어 웹 어플리케이션 서버가 웹 브라우저의 요청을 받을 때마다 해당 함수를 찾아서 호출하게 된다. 실제 함수 매핑은 웹 어플리케이션 서버보다는 웹 어플리케이션 자체(혹은 웹 어플리케이션 프레임워크)에서 발생하는 경우가 많다.\n",
    "\n",
    "웹 어플리케이션은 독자적으로 함수 패키지를 만들기 보다는 웹 어플리케이션 프레임워크라고 불리는 구조 및 기반 클래스를 사용하여 만드는 경우가 대부분이다.\n",
    "\n",
    "## 웹 어플리케이션 프레임워크\n",
    "\n",
    "웹 어플리케이션 프레임워크는 WSGI 웹 어플리케이션을 빠르고 쉽게 개발하기 위한 기반 구조(archtecture) 및 클래스 라이브러리를 말한다.\n",
    "\n",
    "Python으로 구현된 다양한 웹 어플리케이션 프레임워크들이 존재한다. 일부 예를 들면 다음과 같다.\n",
    "\n",
    "* django\n",
    "* flask\n",
    "* pyramids\n",
    "* turbogears\n",
    "* weppy\n",
    "* web2py\n",
    "* bottle\n",
    "* falcon\n",
    "* muffin\n",
    "* pylon\n",
    "* grokk\n",
    "* zope\n",
    "\n",
    "\n",
    "\n",
    "* 웹 어플리케이션 프레임워크 성능 비교: http://klen.github.io/py-frameworks-bench/\n",
    "\n",
    "웹 어플리케이션의 주 기능 중의 하나는 URL과 함수간의 매핑이다. 예를 들어 django의 경우 다음과 같은 URL 설정을 사용하는데\n",
    "\n",
    "```python\n",
    "urlpatterns = [\n",
    "    url(r'^$', views.index, name='index'),\n",
    "    url(r'^(?P<question_id>[0-9]+)/$', views.detail, name='detail'),\n",
    "]\n",
    "```\n",
    "\n",
    "이 설정에 따르면 웹 브라우저에서\n",
    "* http://server/polls/ 를 호출하면 `views` 패키지의 `index` 함수가 호출되고 \n",
    "* http://server/polls/5/ 를 호출하면 `views` 패키지의 `detail` 함수가 `question_id` 인수 5와 함께 호출된다.\n",
    "\n",
    "## ORM (Object Relation Mapper)\n",
    "\n",
    "웹 서비스가 하는 대부분의 일은 데이터 베이스로부터 특정한 데이터를 찾아 보여주는 작업이다. 따라서 데이터 베이스를 쉽게 다울 수 있는 도구가 필수적이다. \n",
    "\n",
    "ORM은 데이터 베이스의 구조 즉, 스키마를 Python 클래스 정의로 구현할 수 있도록 한다. 예를 들어 django의 경우 자체적인 ORM을 사용하는데 다음과 같이 클래스를 정의하게 되면\n",
    "\n",
    "```python\n",
    "class Question(models.Model):\n",
    "    question_text = models.CharField(max_length=200)\n",
    "    pub_date = models.DateTimeField('date published')\n",
    "```\n",
    "\n",
    "데이터 베이스에서 스키마를 지정하여 테이블을 생성하는 SQL을 자동 생성하여 실행시킨다.\n",
    "\n",
    "```\n",
    "CREATE TABLE \"question\" (\n",
    "    \"id\" serial NOT NULL PRIMARY KEY,\n",
    "    \"question_text\" varchar(200) NOT NULL,\n",
    "    \"pub_date\" timestamp with time zone NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Template Engine\n",
    "\n",
    "데이터 베이스로부터 얻어진 데이터는 json이나 txt 같은 단순한 형태로 서비스되기도 하지만 대부분 html 파일을 형태로 서비스된다. 따라서 데이터 요소를 포함하는 html 파일을 동적으로 생성하는 기능을 필요하다. Template Engine은 미리 만들어진 html 파일 template의 일부 문자열을 실제 데이터로 치환(replace)하는 역할을 한다.\n",
    "\n",
    "django의 경우에는 자체 template engine을 가지고 있지만 flask는 jinja2와 같은 외부 파이썬 패키지를 사용한다. \n",
    "\n",
    "예를 들어 jinja2 패키지는 다음과 같은 context와 \n",
    "\n",
    "```\n",
    "contries = [\n",
    "{'Name': 'Afghanistan', 'Population': 22720000},\n",
    "{'Name': 'Albania', 'Population': 3401200},\n",
    "{'Name': 'Algeria', 'Population': 31471000},\n",
    "]\n",
    "```\n",
    "\n",
    "다음 template을 합성하여 \n",
    "\n",
    "```\n",
    "<html>\n",
    "<head>\n",
    "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
    "<title>{{ title }}</title>\n",
    "</head>\n",
    "<body>\n",
    "<table>\n",
    "{% for country in countries %}\n",
    "<tr><td>{{ country['Name'] }}</td><td>{{ country['Population'] }}</td></tr>\n",
    "{% endfor %}\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "다음과 같은 html 을 생성할 수 있다.\n",
    "\n",
    "```\n",
    "<html>\n",
    "<head>\n",
    "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
    "<title>Country list</title>\n",
    "</head>\n",
    "<body>\n",
    "<table>\n",
    "<tr><td>Afghanistan</td><td>22720000</td></tr>\n",
    "<tr><td>Albania</td><td>3401200</td></tr>\n",
    "<tr><td>Algeria</td><td>31471000</td></tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python을 활용한 웹 정보 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib 패키지\n",
    "\n",
    "* `urlencode` : URL 인수 문자열 생성\n",
    "* `urlopen` : 웹서버 연결\n",
    "* `urlretrieve` : 웹서버 연결 및 HTML 문서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urlstr = \"http://www.google.com/finance/historical?q=NASDAQ%3AAAPL&output=csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='http', netloc='www.google.com', path='/finance/historical', params='', query='q=NASDAQ%3AAAPL&output=csv', fragment='')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlobj = urllib.parse.urlparse(urlstr)\n",
    "urlobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['csv'], 'q': ['NASDAQ:AAPL']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.parse.parse_qs(urlobj.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol = \"NASDAQ:NVDA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NASDAQ%3ANVDA'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.parse.quote(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.google.com/finance/historical?q=NASDAQ:NVDA&output=csv'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.google.com/finance/historical?q={}&output=csv\".format(symbol)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xef\\xbb\\xbfDate,Open,High,Low,Close,Volume\\n31-May-17,146.69,147.00,142.05,144.35,22182894\\n30-May-17,143.70,146.29,143.05,144.87,24741298\\n26-May-17,137.93,145.28,137.11,141.84,19478497\\n25-May-17,140.00,140.03,136.44,138.26,15205701\\n24-May-17,140.96,141.07,138.08,138.57,20434495\\n23-May-17,139.70,139.79,135.71,137.03,17031247\\n22-May-17,137.77,139.48,137.33,138.90,20915225\\n19-May-17,137.02,138.22,135.22,136.00,25459271\\n18-May-17,129.50,133.43,127.05,133.07,28900757\\n17-May-17,134.10,134.86,127.55,127.72,31784158\\n16-May-17,136.38,137.44,133.36,136.81,28017471\\n15-May-17,129.56,134.41,129.38,134.31,27188548\\n12-May-17,126.63,129.60,125.78,127.89,24065459\\n11-May-17,120.05,130.43,119.91,126.50,48494808\\n10-May-17,114.29,121.82,114.02,121.29,53227434\\n9-May-17,103.00,104.93,102.66,102.94,21191792\\n8-May-17,104.34,104.40,102.31,102.77,9130990\\n5-May-17,103.38,104.15,102.75,103.86,5710736\\n4-May-17,104.50,104.95,103.53,103.85,5244692\\n3-May-17,103.20,104.64,102.60,104.25,8422633\\n2-May-17,105.55,105.60,102.56,103.'\n"
     ]
    }
   ],
   "source": [
    "data = urllib.request.urlopen(url).read()\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\JIHOON~1\\\\AppData\\\\Local\\\\Temp\\\\tmp4kyokvxd',\n",
       " <http.client.HTTPMessage at 0x151cce035c0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests 패키지\n",
    "\n",
    "* http://docs.python-requests.org/en/master/\n",
    "* HTTP protocols (get, post, put, delete, head, options)\n",
    "\n",
    "\n",
    "https://www.google.com/finance/historical?q=KRX%3AKOSPI200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html><head><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp(\"CSI/\"+a)}catch(h){}};this.tick(\"start\",null,a)}var a;if(window.performance)var d=(a=window.performance.timing)&&a.responseStart;var f=0<d?new e(d):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick(\"_wtsrt\",void 0,c),b.tick(\"wtsrt_\",\"_wtsrt\",\n",
      "d),b.tick(\"tbsd_\",\"wtsrt_\"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick(\"_tbnd\",void 0,window.chrome.csi().startE),b.tick(\"tbnd_\",\"_tbnd\",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick(\"_tbnd\",void 0,window.external.startE),b.tick(\"tbnd_\",\"_tbnd\",c))),a&&(window.jstiming.pt=a)}catch(g){}})();}).cal\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://www.google.com/finance/historical?q=KRX%3AKOSPI200\"\n",
    "req = requests.get(url)\n",
    "print(req.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beautifulsoup 패키지\n",
    "\n",
    "* https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* HTML 문서 파싱 및 태그 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(req.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"gf-table historical_price\">\n",
      " <tr class=\"bb\">\n",
      "  <th class=\"bb lm lft\">\n",
      "   Date\n",
      "  </th>\n",
      "  <th class=\"rgt bb\">\n",
      "   Open\n",
      "  </th>\n",
      "  <th class=\"rgt bb\">\n",
      "   High\n",
      "  </th>\n",
      "  <th class=\"rgt bb\">\n",
      "   Low\n",
      "  </th>\n",
      "  <th class=\"rgt bb\">\n",
      "   Close\n",
      "  </th>\n",
      "  <th class=\"rgt bb rm\">\n",
      "   Volume\n",
      "  </th>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td class=\"lm\">\n",
      "   May 31, 2017\n",
      "  </td>\n",
      "  <td class=\"rgt\">\n",
      "   303.79\n",
      "  </td>\n",
      "  <td class=\"rgt\">\n",
      "   306.18\n",
      "  </td>\n",
      "  <td class=\"rgt\">\n",
      "   303.64\n",
      "  </td>\n",
      "  <td class=\"rgt\">\n",
      "   304.67\n",
      "  </td>\n",
      "  <td class=\"rgt rm\">\n",
      "   114,684,000\n",
      "  </td>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td class=\"lm\">\n",
      "   May 30, 2017\n",
      "  </td>\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "table = soup.find(\"table\", class_=\"gf-table historical_price\")\n",
    "print(table.prettify()[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"lm\">May 30, 2017\n",
       " </td>, <td class=\"rgt\">306.89\n",
       " </td>, <td class=\"rgt\">306.99\n",
       " </td>, <td class=\"rgt\">303.11\n",
       " </td>, <td class=\"rgt\">304.59\n",
       " </td>, <td class=\"rgt rm\">94,122,000\n",
       " </td>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.find_all('tr')[2].find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dateutil\n",
    "\n",
    "list_records = []\n",
    "\n",
    "for i, r in enumerate(table.find_all('tr')):\n",
    "    record = None\n",
    "    for j, c in enumerate(r.find_all('td')):\n",
    "        if j == 0:\n",
    "            record = {\"date\": dateutil.parser.parse(c.text.strip())}\n",
    "        elif j == 1:\n",
    "            record.update({\"open\": float(c.text.strip())})\n",
    "        elif j == 2:\n",
    "            record.update({\"high\": float(c.text.strip())})\n",
    "        elif j == 3:\n",
    "            record.update({\"low\": float(c.text.strip())})\n",
    "        elif j == 4:\n",
    "            record.update({\"close\": float(c.text.strip())})\n",
    "        elif j == 5:\n",
    "            record.update({\"volume\": int(c.text.strip().replace(',',''))})\n",
    "    if record is not None:\n",
    "        list_records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'close': 304.67,\n",
       " 'date': datetime.datetime(2017, 5, 31, 0, 0),\n",
       " 'high': 306.18,\n",
       " 'low': 303.64,\n",
       " 'open': 303.79,\n",
       " 'volume': 114684000}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>276.32</td>\n",
       "      <td>278.04</td>\n",
       "      <td>275.75</td>\n",
       "      <td>277.76</td>\n",
       "      <td>69505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-04-19</td>\n",
       "      <td>277.66</td>\n",
       "      <td>278.07</td>\n",
       "      <td>275.97</td>\n",
       "      <td>276.49</td>\n",
       "      <td>65259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>279.36</td>\n",
       "      <td>279.42</td>\n",
       "      <td>277.05</td>\n",
       "      <td>278.23</td>\n",
       "      <td>54714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-04-17</td>\n",
       "      <td>278.06</td>\n",
       "      <td>279.06</td>\n",
       "      <td>277.49</td>\n",
       "      <td>278.10</td>\n",
       "      <td>47713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>277.59</td>\n",
       "      <td>278.14</td>\n",
       "      <td>276.12</td>\n",
       "      <td>277.31</td>\n",
       "      <td>44577000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date    open    high     low   close    volume\n",
       "25 2017-04-20  276.32  278.04  275.75  277.76  69505000\n",
       "26 2017-04-19  277.66  278.07  275.97  276.49  65259000\n",
       "27 2017-04-18  279.36  279.42  277.05  278.23  54714000\n",
       "28 2017-04-17  278.06  279.06  277.49  278.10  47713000\n",
       "29 2017-04-14  277.59  278.14  276.12  277.31  44577000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list_records, columns=[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lxml 패키지\n",
    "\n",
    "* http://lxml.de/index.html\n",
    "* xpath 사용 가능\n",
    " * https://en.wikipedia.org/wiki/XPath\n",
    " * https://www.w3schools.com/xml/xpath_intro.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "import numpy as np\n",
    "tree = lxml.html.fromstring(req.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dateutil\n",
    "dates = [dateutil.parser.parse(x.text.strip()) for x in tree.xpath('//td[@class=\"lm\"]')]\n",
    "prices = np.reshape([float(x.text.strip()) for x in tree.xpath('//td[@class=\"rgt\"]')], (-1, 4))\n",
    "volumes = np.array([int(x.text.strip().replace(',','')) for x in tree.xpath('//td[@class=\"rgt rm\"]')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2017, 5, 31, 0, 0),\n",
       " datetime.datetime(2017, 5, 30, 0, 0),\n",
       " datetime.datetime(2017, 5, 29, 0, 0),\n",
       " datetime.datetime(2017, 5, 26, 0, 0),\n",
       " datetime.datetime(2017, 5, 25, 0, 0)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 303.79,  306.18,  303.64,  304.67],\n",
       "       [ 306.89,  306.99,  303.11,  304.59],\n",
       "       [ 307.98,  309.32,  305.13,  306.52],\n",
       "       [ 305.41,  308.51,  305.07,  306.96],\n",
       "       [ 302.83,  305.34,  302.18,  305.22]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>276.32</td>\n",
       "      <td>278.04</td>\n",
       "      <td>275.75</td>\n",
       "      <td>277.76</td>\n",
       "      <td>69505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-04-19</td>\n",
       "      <td>277.66</td>\n",
       "      <td>278.07</td>\n",
       "      <td>275.97</td>\n",
       "      <td>276.49</td>\n",
       "      <td>65259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>279.36</td>\n",
       "      <td>279.42</td>\n",
       "      <td>277.05</td>\n",
       "      <td>278.23</td>\n",
       "      <td>54714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-04-17</td>\n",
       "      <td>278.06</td>\n",
       "      <td>279.06</td>\n",
       "      <td>277.49</td>\n",
       "      <td>278.10</td>\n",
       "      <td>47713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>277.59</td>\n",
       "      <td>278.14</td>\n",
       "      <td>276.12</td>\n",
       "      <td>277.31</td>\n",
       "      <td>44577000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date    open    high     low   close    volume\n",
       "25 2017-04-20  276.32  278.04  275.75  277.76  69505000\n",
       "26 2017-04-19  277.66  278.07  275.97  276.49  65259000\n",
       "27 2017-04-18  279.36  279.42  277.05  278.23  54714000\n",
       "28 2017-04-17  278.06  279.06  277.49  278.10  47713000\n",
       "29 2017-04-14  277.59  278.14  276.12  277.31  44577000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_o = prices[:,0]\n",
    "price_h = prices[:,1]\n",
    "price_l = prices[:,2]\n",
    "price_c = prices[:,3]\n",
    "\n",
    "df = pd.DataFrame({\"date\": dates, \"open\": price_o, \"high\": price_h, \"low\": price_l, \"close\": price_c, \"volume\": volumes},\n",
    "                  columns=[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy를 사용한 웹 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy는 다수의 프로세스를 동시에 가동하여 크롤링 효율을 높이고 데이터 베이스에 기록까지 할 수 있는 웹 크롤링용 파이썬 패키지이다.\n",
    "\n",
    "* http://doc.scrapy.org/en/latest/index.html\n",
    "\n",
    "\n",
    "Scrapy를 이용한 웹 크롤링 어플리케이션은 다음과 같은 순서로 개발한다.\n",
    "\n",
    "* Scrapy shell을 이용한 문서 구조 파악\n",
    "* Scrapy 프로젝트 생성\n",
    "* Spider 클래스 구현\n",
    "* Item 클래스 구현\n",
    "* Pipeline 구현\n",
    "* Setting 설정\n",
    "\n",
    "\n",
    "## Scrapy shell\n",
    "\n",
    "scrapy shell은 콘솔에서 실행가능한 shell 도구이다. 크롤링하고자 하는 웹사이트 url을 인수로 가진다. 예를 들어 https://www.google.com/finance/historical?q=KRX%3AKOSPI200 페이지를 접근하려면 다음과 같이 실행한다.\n",
    "\n",
    "```python\n",
    "$ scrapy shell https://www.google.com/finance/historical?q=KRX%3AKOSPI200\n",
    "```\n",
    "\n",
    "실행하면 다음과 같은 페이지가 나타나며 ipython 콘솔이 실행된다.\n",
    "\n",
    "\n",
    "```\n",
    "2016-07-07 08:28:13 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
    "2016-07-07 08:28:13 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
    "2016-07-07 08:28:13 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0}\n",
    "2016-07-07 08:28:13 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, CoreStats, SpiderState\n",
    "2016-07-07 08:28:13 [boto] DEBUG: Retrieving credentials from metadata server.\n",
    "2016-07-07 08:28:13 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
    "2016-07-07 08:28:13 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
    "2016-07-07 08:28:13 [scrapy] INFO: Enabled item pipelines:\n",
    "2016-07-07 08:28:13 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "2016-07-07 08:28:13 [scrapy] INFO: Spider opened\n",
    "2016-07-07 08:28:14 [scrapy] DEBUG: Crawled (200) <GET https://www.google.com/finance/historical?q=KRX%3AKOSPI200> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7fcd973f0d90>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "[s]   response   <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7fcd94f80b90>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7fcd8d9b2f50>\n",
    "[s] Useful shortcuts:\n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n",
    "[s]   view(response)    View response in a browser\n",
    "2016-07-07 08:28:14 [root] DEBUG: Using default logger\n",
    "2016-07-07 08:28:14 [root] DEBUG: Using default logger\n",
    "\n",
    "In [1]:\n",
    "\n",
    "```\n",
    "\n",
    "이 ipython 콘솔은 다음과 같은 객체들을 이미 생성해 놓은 상태이다. 웹서버의 응답 즉, 웹페이지 내용은 `response` 객체에 저장되어 있다.\n",
    "\n",
    "* `crawler`\n",
    "* `request`\n",
    "* `response`\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "In [1]: type(response)\n",
    "Out[1]: scrapy.http.response.html.HtmlResponse\n",
    "\n",
    "In [2]: response.url\n",
    "Out[2]: 'https://www.google.com/finance/historical?q=KRX%3AKOSPI200'\n",
    "\n",
    "In [3]: response.body[:100]\n",
    "Out[3]: '<!DOCTYPE html><html><head><script>(function(){(function(){function e(a){this.t={};this.tick=functio'\n",
    "\n",
    "```\n",
    "\n",
    " `response` 객체 즉, `scrapy.http.response.html.HtmlResponse` 클래스는 HTML 파싱을 위한 `xpath` 등의 메서드를 제공한다. 이를 이용하면 원하는 html 요소를 선택할 수 있다.\n",
    " \n",
    " * http://doc.scrapy.org/en/latest/topics/request-response.html?#response-objects\n",
    "\n",
    "```\n",
    "In [4]: response.xpath('//td[@class=\"lm\"]')\n",
    "Out[4]:\n",
    "[<Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jul 6, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jul 5, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jul 4, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jul 1, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 30, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 29, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 28, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 27, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 24, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 23, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 22, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 21, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 20, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 17, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 16, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 15, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 14, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 13, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 10, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 9, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 8, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 7, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 3, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 2, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">Jun 1, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">May 31, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">May 30, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">May 27, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">May 26, 2016\\n</td>'>,\n",
    " <Selector xpath='//td[@class=\"lm\"]' data=u'<td class=\"lm\">May 25, 2016\\n</td>'>]\n",
    "\n",
    "```\n",
    "\n",
    "## Scrapy 프로젝트 생성\n",
    "\n",
    "scrapy shell 을 사용하여 원하는 요소에 대한 조사가 끝나면 실제로 scrapy 를 구현해야 한다. 첫번재 단계로 프로젝트를 생성한다.\n",
    "\n",
    "```\n",
    "scrapy startproject tutorial\n",
    "```\n",
    "\n",
    "이 `tutorial` 프로젝트를 담고 있는 다음과 같이 디렉토리가 생성된다\n",
    "\n",
    "```\n",
    "tutorial/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "        items.py          # project items file\n",
    "        pipelines.py      # project pipelines file\n",
    "        settings.py       # project settings file\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py\n",
    "```\n",
    "\n",
    "\n",
    "## Spider 클래스 구현\n",
    "\n",
    "`spiders` 디렉토리 아래에는 실제로 웹 페이지를 읽고 데이터를 반환하는 클래스를 구현한다.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# __init__.py\n",
    "\n",
    "from dailystock import *\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# dailystock.py\n",
    "\n",
    "import scrapy\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "\n",
    "class DailyStockSpider(scrapy.Spider):\n",
    "    name = \"dailystock\"\n",
    "    start_urls = [\"https://www.google.com/finance/historical?q=KRX%3AKOSPI200\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        dates = [parse(x.extract().strip()) for x in response.xpath('//td[@class=\"lm\"]/text()')]\n",
    "        volumes = np.array([int(x.extract().strip().replace(',','')) for x in response.xpath('//td[@class=\"rgt rm\"]/text()')])\n",
    "        prices = np.reshape([float(x.extract().strip()) for x in response.xpath('//td[@class=\"rgt\"]/text()')], (-1, 4))\n",
    "        for d, v, p in zip(dates, volumes, prices):\n",
    "          symbol = \"KOSPI\"\n",
    "          date = d\n",
    "          price_open = p[0]\n",
    "          price_high = p[1]\n",
    "          price_low = p[2]\n",
    "          price_close = p[3]\n",
    "          volume = v\n",
    "          yield {\"symbol\": symbol, \"date\": date, \n",
    "                 \"price_open\": price_open, \"price_high\": price_high, \n",
    "                 \"price_low\": price_low, \"price_close\": price_close, \n",
    "                 \"volume\": volume}\n",
    "```\n",
    "\n",
    "일단 spider가 구현되면 다음과 같이 크롤링을 할 수 있다. 이 명령은 프로젝트 디렉토리 아래에서 실행해야 한다.\n",
    "\n",
    "```\n",
    "scrapy crawl dailystock -o data.json\n",
    "```\n",
    "\n",
    "## Item 클래스 구현\n",
    "\n",
    "`items.py` 파일내에는 데이터베이스 레코드를 구현한다.\n",
    "\n",
    "```\n",
    "import scrapy\n",
    "\n",
    "class DailyStockItem(scrapy.Item):\n",
    "    symbol = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    price_open = scrapy.Field()\n",
    "    price_high = scrapy.Field()\n",
    "    price_low = scrapy.Field()\n",
    "    price_close = scrapy.Field()\n",
    "    volume = scrapy.Field()\n",
    "```\n",
    "\n",
    "## Pipeline 구현\n",
    "\n",
    "pipeline은 수집한 데이터를 파일이 아닌 데이터베이스에 직접 넣기 위한 것이다. `pipelines.py` 파일에 구현한다. 보통 생성자에서 데이터베이스 연결을 만들고 `process_item` 메서드에서 레코드 입력 및 커밋(commit)을 한다. \n",
    "\n",
    "여기에서는 sqlite 데이터베이스를 사용하였다.\n",
    "\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "\n",
    "class DailyStockPipeline(object):\n",
    "    filename = 'dailystock.sqlite'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conn = None\n",
    "        if os.path.exists(self.filename):\n",
    "            self.conn = sqlite3.connect(self.filename)\n",
    "        else:\n",
    "            self.conn = sqlite3.connect(self.filename)\n",
    "            self.conn.execute(\"\"\"create table dailystock\n",
    "                (symbol TEXT NOT NULL,\n",
    "                 date TIMESTAMP NOT NULL,\n",
    "                 price_open REAL,\n",
    "                 price_high REAL,\n",
    "                 price_low REAL,\n",
    "                 price_close REAL,\n",
    "                 volume INTEGER,\n",
    "                 PRIMARY KEY (symbol, date))\"\"\")\n",
    "            self.conn.commit()\n",
    "\n",
    "    def process_item(self, item, domain):\n",
    "        try:\n",
    "            self.conn.execute('insert into dailystock values(?,?,?,?,?,?,?)',\n",
    "                (item['symbol'], item['date'],\n",
    "                 item['price_open'], item['price_high'],\n",
    "                 item['price_low'], item['price_close'],\n",
    "                 item['volume']))\n",
    "            self.conn.commit()\n",
    "        except Exception, e:\n",
    "            print str(e)\n",
    "        return item\n",
    "```\n",
    "\n",
    "## Setting 설정\n",
    "\n",
    "이 pipeline을 사용하기 위해서는 settings.py 파일에 다음과 같이 설정을 추가해야 한다.\n",
    "\n",
    "```python\n",
    "BOT_NAME = 'tutorial'\n",
    "SPIDER_MODULES = ['tutorial.spiders']\n",
    "NEWSPIDER_MODULE = 'tutorial.spiders'\n",
    "ITEM_PIPELINES = {\n",
    "    'tutorial.pipelines.DailyStockPipeline': 300,\n",
    "}\n",
    "DOWNLOAD_HANDLERS = {\n",
    "    's3': None,\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 크롤링 \n",
    "\n",
    "실제로 크롤링을 하려면 tutorial 프로젝트 디렉토리에서 다음과 같이 명령한다.\n",
    "\n",
    "```\n",
    "$ scrapy crawl dailystock\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Scrapy 1.0.3 started (bot: tutorial)\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial'}\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Enabled item pipelines: DailyStockPipeline\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Spider opened\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Crawled (200) <GET https://www.google.com/finance/historical?q=KRX%3AKOSPI200> (referer: None)\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 244.59999999999999, 'volume': 62210000, 'price_open': 243.15000000000001, 'price_low': 242.63, 'date': datetime.datetime(2016, 7, 7, 0, 0), 'price_high': 245.02000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 241.86000000000001, 'volume': 71672000, 'price_open': 245.37, 'price_low': 240.72999999999999, 'date': datetime.datetime(2016, 7, 6, 0, 0), 'price_high': 245.74000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 246.91, 'volume': 54810000, 'price_open': 247.66999999999999, 'price_low': 246.50999999999999, 'date': datetime.datetime(2016, 7, 5, 0, 0), 'price_high': 247.84}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 247.62, 'volume': 63633000, 'price_open': 246.66, 'price_low': 246.16, 'date': datetime.datetime(2016, 7, 4, 0, 0), 'price_high': 247.94}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 246.52000000000001, 'volume': 63045000, 'price_open': 244.96000000000001, 'price_low': 244.78, 'date': datetime.datetime(2016, 7, 1, 0, 0), 'price_high': 247.56999999999999}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 244.13999999999999, 'volume': 75453000, 'price_open': 244.25, 'price_low': 242.52000000000001, 'date': datetime.datetime(2016, 6, 30, 0, 0), 'price_high': 244.47999999999999}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 242.31999999999999, 'volume': 67355000, 'price_open': 241.28999999999999, 'price_low': 240.68000000000001, 'date': datetime.datetime(2016, 6, 29, 0, 0), 'price_high': 243.71000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 240.08000000000001, 'volume': 79648000, 'price_open': 236.78, 'price_low': 236.72999999999999, 'date': datetime.datetime(2016, 6, 28, 0, 0), 'price_high': 240.46000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 239.28, 'volume': 95450000, 'price_open': 236.78999999999999, 'price_low': 236.68000000000001, 'date': datetime.datetime(2016, 6, 27, 0, 0), 'price_high': 239.28}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 239.21000000000001, 'volume': 190032000, 'price_open': 248.22999999999999, 'price_low': 234.97, 'date': datetime.datetime(2016, 6, 24, 0, 0), 'price_high': 248.27000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 246.31, 'volume': 65983000, 'price_open': 246.41999999999999, 'price_low': 245.63999999999999, 'date': datetime.datetime(2016, 6, 23, 0, 0), 'price_high': 246.81}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 246.75, 'volume': 65848000, 'price_open': 245.16999999999999, 'price_low': 244.78999999999999, 'date': datetime.datetime(2016, 6, 22, 0, 0), 'price_high': 247.03}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 245.34, 'volume': 58402000, 'price_open': 244.59999999999999, 'price_low': 243.99000000000001, 'date': datetime.datetime(2016, 6, 21, 0, 0), 'price_high': 245.5}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 245.16999999999999, 'volume': 75085000, 'price_open': 244.44, 'price_low': 243.74000000000001, 'date': datetime.datetime(2016, 6, 20, 0, 0), 'price_high': 245.63}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 241.63, 'volume': 77334000, 'price_open': 243.34999999999999, 'price_low': 241.31999999999999, 'date': datetime.datetime(2016, 6, 17, 0, 0), 'price_high': 244.02000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 241.61000000000001, 'volume': 83308000, 'price_open': 243.66, 'price_low': 240.52000000000001, 'date': datetime.datetime(2016, 6, 16, 0, 0), 'price_high': 244.0}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 243.30000000000001, 'volume': 84095000, 'price_open': 243.41, 'price_low': 242.19, 'date': datetime.datetime(2016, 6, 15, 0, 0), 'price_high': 244.16}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 243.34999999999999, 'volume': 107840000, 'price_open': 243.78999999999999, 'price_low': 242.36000000000001, 'date': datetime.datetime(2016, 6, 14, 0, 0), 'price_high': 244.46000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 244.05000000000001, 'volume': 79114000, 'price_open': 246.74000000000001, 'price_low': 243.69999999999999, 'date': datetime.datetime(2016, 6, 13, 0, 0), 'price_high': 246.96000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 248.96000000000001, 'volume': 89452000, 'price_open': 249.86000000000001, 'price_low': 248.63, 'date': datetime.datetime(2016, 6, 10, 0, 0), 'price_high': 249.86000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 250.19, 'volume': 138770000, 'price_open': 250.19, 'price_low': 248.46000000000001, 'date': datetime.datetime(2016, 6, 9, 0, 0), 'price_high': 251.50999999999999}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 250.03999999999999, 'volume': 90733000, 'price_open': 248.24000000000001, 'price_low': 247.68000000000001, 'date': datetime.datetime(2016, 6, 8, 0, 0), 'price_high': 250.03999999999999}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 247.84999999999999, 'volume': 79435000, 'price_open': 245.55000000000001, 'price_low': 245.53999999999999, 'date': datetime.datetime(2016, 6, 7, 0, 0), 'price_high': 247.86000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 244.38999999999999, 'volume': 86345000, 'price_open': 244.94, 'price_low': 243.66999999999999, 'date': datetime.datetime(2016, 6, 3, 0, 0), 'price_high': 244.94}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 244.18000000000001, 'volume': 78953000, 'price_open': 243.88, 'price_low': 243.31, 'date': datetime.datetime(2016, 6, 2, 0, 0), 'price_high': 244.66}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 243.58000000000001, 'volume': 78835000, 'price_open': 242.69999999999999, 'price_low': 242.47999999999999, 'date': datetime.datetime(2016, 6, 1, 0, 0), 'price_high': 244.19}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 243.63, 'volume': 117966000, 'price_open': 241.03, 'price_low': 240.36000000000001, 'date': datetime.datetime(2016, 5, 31, 0, 0), 'price_high': 243.84}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 241.72999999999999, 'volume': 67446000, 'price_open': 241.94, 'price_low': 240.41, 'date': datetime.datetime(2016, 5, 30, 0, 0), 'price_high': 242.06999999999999}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 241.84999999999999, 'volume': 92225000, 'price_open': 241.25, 'price_low': 240.74000000000001, 'date': datetime.datetime(2016, 5, 27, 0, 0), 'price_high': 242.41}\n",
    "2016-07-08 03:40:40 [scrapy] DEBUG: Scraped from <200 https://www.google.com/finance/historical?q=KRX%3AKOSPI200>\n",
    "{'symbol': 'KOSPI', 'price_close': 240.58000000000001, 'volume': 130282000, 'price_open': 241.56, 'price_low': 240.44999999999999, 'date': datetime.datetime(2016, 5, 26, 0, 0), 'price_high': 242.05000000000001}\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Closing spider (finished)\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 247,\n",
    " 'downloader/request_count': 1,\n",
    " 'downloader/request_method_count/GET': 1,\n",
    " 'downloader/response_bytes': 8127,\n",
    " 'downloader/response_count': 1,\n",
    " 'downloader/response_status_count/200': 1,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2016, 7, 8, 3, 40, 40, 723470),\n",
    " 'item_scraped_count': 30,\n",
    " 'log_count/DEBUG': 32,\n",
    " 'log_count/INFO': 7,\n",
    " 'response_received_count': 1,\n",
    " 'scheduler/dequeued': 1,\n",
    " 'scheduler/dequeued/memory': 1,\n",
    " 'scheduler/enqueued': 1,\n",
    " 'scheduler/enqueued/memory': 1,\n",
    " 'start_time': datetime.datetime(2016, 7, 8, 3, 40, 40, 271766)}\n",
    "2016-07-08 03:40:40 [scrapy] INFO: Spider closed (finished)\n",
    "```\n",
    "\n",
    "크롤링이 완료되면 다음과 같이 sqlite 데이터베이스를 확인할 수 있다.\n",
    "\n",
    "```\n",
    "$ sqlite3 dailystock.sqlite 'select * from dailystock'\n",
    "KOSPI|2016-07-07 00:00:00|243.15|245.02|242.63|244.6|62210000\n",
    "KOSPI|2016-07-06 00:00:00|245.37|245.74|240.73|241.86|71672000\n",
    "KOSPI|2016-07-05 00:00:00|247.67|247.84|246.51|246.91|54810000\n",
    "KOSPI|2016-07-04 00:00:00|246.66|247.94|246.16|247.62|63633000\n",
    "KOSPI|2016-07-01 00:00:00|244.96|247.57|244.78|246.52|63045000\n",
    "KOSPI|2016-06-30 00:00:00|244.25|244.48|242.52|244.14|75453000\n",
    "KOSPI|2016-06-29 00:00:00|241.29|243.71|240.68|242.32|67355000\n",
    "KOSPI|2016-06-28 00:00:00|236.78|240.46|236.73|240.08|79648000\n",
    "KOSPI|2016-06-27 00:00:00|236.79|239.28|236.68|239.28|95450000\n",
    "KOSPI|2016-06-24 00:00:00|248.23|248.27|234.97|239.21|190032000\n",
    "KOSPI|2016-06-23 00:00:00|246.42|246.81|245.64|246.31|65983000\n",
    "KOSPI|2016-06-22 00:00:00|245.17|247.03|244.79|246.75|65848000\n",
    "KOSPI|2016-06-21 00:00:00|244.6|245.5|243.99|245.34|58402000\n",
    "KOSPI|2016-06-20 00:00:00|244.44|245.63|243.74|245.17|75085000\n",
    "KOSPI|2016-06-17 00:00:00|243.35|244.02|241.32|241.63|77334000\n",
    "KOSPI|2016-06-16 00:00:00|243.66|244.0|240.52|241.61|83308000\n",
    "KOSPI|2016-06-15 00:00:00|243.41|244.16|242.19|243.3|84095000\n",
    "KOSPI|2016-06-14 00:00:00|243.79|244.46|242.36|243.35|107840000\n",
    "KOSPI|2016-06-13 00:00:00|246.74|246.96|243.7|244.05|79114000\n",
    "KOSPI|2016-06-10 00:00:00|249.86|249.86|248.63|248.96|89452000\n",
    "KOSPI|2016-06-09 00:00:00|250.19|251.51|248.46|250.19|138770000\n",
    "KOSPI|2016-06-08 00:00:00|248.24|250.04|247.68|250.04|90733000\n",
    "KOSPI|2016-06-07 00:00:00|245.55|247.86|245.54|247.85|79435000\n",
    "KOSPI|2016-06-03 00:00:00|244.94|244.94|243.67|244.39|86345000\n",
    "KOSPI|2016-06-02 00:00:00|243.88|244.66|243.31|244.18|78953000\n",
    "KOSPI|2016-06-01 00:00:00|242.7|244.19|242.48|243.58|78835000\n",
    "KOSPI|2016-05-31 00:00:00|241.03|243.84|240.36|243.63|117966000\n",
    "KOSPI|2016-05-30 00:00:00|241.94|242.07|240.41|241.73|67446000\n",
    "KOSPI|2016-05-27 00:00:00|241.25|242.41|240.74|241.85|92225000\n",
    "KOSPI|2016-05-26 00:00:00|241.56|242.05|240.45|240.58|130282000\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
