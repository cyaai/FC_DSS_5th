{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:09:58) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 기초 이론\n",
    "\n",
    "신경망(neural network) 모형은 기저 함수(basis function)도 모수(parameter) 값에 의해 변화할 수 있는 적응형 기저 함수 모형(adaptive basis function model)이며 구조적으로는 여러개의 퍼셉트론을 쌓아놓은 형태이므로 MLP(multi-layer perceptron)으로도 불린다.\n",
    "\n",
    "## 퍼셉트론 복습\n",
    "\n",
    "다음 그림과 같이 독립 변수 벡터가 3차원인 간단한 퍼셉트론 모형을 가정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입력 $x$\n",
    " $$ x_1,x_2,x_3 $$\n",
    " \n",
    "* 가중치  $w$\n",
    " $$ w_1, w_2, w_3 $$\n",
    "\n",
    "* 바이어스(y 절편) $b$\n",
    "$$b$$\n",
    "\n",
    "* 활성화 함수 입력값\n",
    " $$ a = \\sum_{j=1}^3 w_j x_j + b $$\n",
    "\n",
    "* 활성화 함수(activation function) $h$ 와 활성화 함수 출력값\n",
    " $$ z = h(a) = h \\left( \\sum_{j=1}^3 w_j x_j + b \\right) $$\n",
    "\n",
    "* 최종 출력 $y$\n",
    "$$\n",
    "y = z\n",
    "$$\n",
    "\n",
    "이런 퍼셉트론에서 $x$ 대신 기저 함수를 적용한 $\\phi(x)$를 사용하면 XOR 문제 등의 비선형 문제를 해결할 수 있다. 그러나 고정된 기저 함수를 사용해야 하므로 문제에 맞는 기저 함수를 찾아야 한다는 단점이 있다.\n",
    "\n",
    "만약 기저 함수 $\\phi(x)$의 형태를 추가적인 모수 $w^{(1)}$, $b^{(1)}$를 사용하여 조절할 수 있다면 즉, 기저함수 $\\phi(x;w^{(1)}, b^{(1)})$ 를 사용하면 $w^{(1)}$ 값을 바꾸는 것만으로 다양한 기저 함수를 시도할 수 있다.\n",
    "\n",
    "\n",
    " $$ z = h \\left(  \\sum_{j=1} w_j^{(2)} \\phi_j(x ; w^{(1)}_j, b^{(1)}_j) + b^{(2)} \\right) $$\n",
    "\n",
    "신경망은 다음과 같이 원래 퍼셉트론과 같은 형태의 적응형 기저함수를 사용한 모형이다.\n",
    "\n",
    "$$ \\phi_j(x ; w^{(1)}_j, b^{(1)}_j)  = h \\left(  \\sum_{i=1} w_{ji}^{(1)} x_i + b_j^{(1)} \\right)  $$\n",
    "\n",
    "즉 전체 모형은 다음과 같다.\n",
    "\n",
    "$$ z = h \\left(  \\sum_{j=1} w_j^{(2)} h \\left(  \\sum_{i=1} w_{ji}^{(1)} x_i + b_j^{(1)} \\right)  + b^{(2)} \\right) $$\n",
    "\n",
    "일반적으로 활성화 함수 $h$ 는 위 아래가 막혀있는(bounded) 시그모이드 함수 $\\sigma$를 사용하는데 가장 많이 사용하는 활성화 함수는 다음과 같은 로지스틱 함수이다.\n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray} \n",
    "z \n",
    "&=& \\sigma(a) \\\\\n",
    "&\\equiv& \\frac{1}{1+e^{-a}}  \\\\\n",
    "&=& \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}\n",
    "\\end{eqnarray} \n",
    "$$\n",
    "\n",
    "이 시그모이드 함수의 특징은 다음과 같은 미분값을 가진다는 것이다. 따라서 미분 계산이 쉽다.\n",
    "\n",
    "$$ \\dfrac{d\\sigma}{da} = \\sigma(1-\\sigma) $$\n",
    "\n",
    "\n",
    "## 퍼셉트론을 사용한 XOR 문제 해결법\n",
    "\n",
    "퍼셉트론을 연속적으로 연결하여 비선형 문제를 해결하는 방법은 이미 디지털 회로 설계에서 사용되던 방법이다.\n",
    "\n",
    "퍼셉트론의 가중치를 적절히 조정하면 다음과 같은 AND / OR 등의 디지털 게이트(gate)를 제작할 수 있다.\n",
    "\n",
    "예를 들어 $w_1 = -2$, $w_2 = -2$, $b = 3$ 인 퍼셉트론은 NAND 게이트를 구현한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table style=\"display: inline-table; margin-right: 30pt;\">\n",
    "<tbody><tr style=\"background:#def; text-align:center;\">\n",
    "<td colspan=\"2\" style=\"text-align:center;\"><b>INPUT</b></td>\n",
    "<td colspan=\"3\" style=\"text-align:center;\"><b>OUTPUT</b></td>\n",
    "</tr>\n",
    "<tr style=\"background:#def; text-align:center;\">\n",
    "<td>A</td>\n",
    "<td>B</td>\n",
    "<td>A AND B</td>\n",
    "<td>A NAND B</td>\n",
    "<td>A XOR B</td>\n",
    "</tr>\n",
    "<tr style=\"background:#dfd; text-align:center;\">\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr style=\"background:#dfd; text-align:center;\">\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "<tr style=\"background:#dfd; text-align:center;\">\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "<tr style=\"background:#dfd; text-align:center;\">\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "</tbody></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_1 = 0$, $x_2 = 0$\n",
    " * $ (−2)\\times 0+(−2)\\times 0+3=3 > 0 \\rightarrow 1$\n",
    "* $x_1 = 0$, $x_2 = 1$\n",
    " * $ (−2)\\times 0+(−2)\\times 1+3=1 > 0 \\rightarrow 1$\n",
    "* $x_1 = 1$, $x_2 = 0$\n",
    " * $ (−2)\\times 1+(−2)\\times 0+3=1 > 0 \\rightarrow 1$\n",
    "* $x_1 = 1$, $x_2 = 1$\n",
    " * $ (−2)\\times 1+(−2)\\times 1+3=-1 < 0 \\rightarrow 0$\n",
    "\n",
    "\n",
    "디지털 회로에서는 복수개의 NAND 게이트를 조합하면 어떤 디지털 로직이라도 구현 가능하다. 예를 들어 다음 회로는 두 입력 신호의 합과 자릿수를 반환하는 반가산기(half adder) 회로이다.\n",
    "\n",
    "<img src=\"https://datascienceschool.net/upfiles/3002b65c9f034818a318ad7f6b09671f.png\">\n",
    "\n",
    "이 퍼셉트론 조합을 보면 4개의 퍼셉트론을 연결하여 XOR 로직을 구현하였음을 알 수 있다.\n",
    "\n",
    "## 다계층 퍼셉트론 (MLP: Multi-Layer Perceptrons)\n",
    "\n",
    "신경망은 퍼셉트론을 여러개 연결한 것으로 다계층 퍼셉트론(MLP: Multi-Layer Perceptrons)이라고도 한다. 신경망에 속한 퍼셉트론은 뉴론(neuron) 또는 노드(node)라고 불린다.\n",
    "\n",
    "각 계층(layer)은 다음 계층에 대해 적응형 기저 함수의 역할을 한다. 최초의 계층은 입력 계층(input layer), 마지막 계측은 출력 계층(output layer)이라고 하며 중간은 은닉 계층(hidden layer)라고 한다.\n",
    "\n",
    "<img src=\"https://datascienceschool.net/upfiles/4dcef7b75de64023900c7f7edb7cbb2f.png\">\n",
    "\n",
    "MLP의 또다른 특징은 출력 계층에 복수개의 출력 뉴런를 가지고 각 뉴런값으로 출력 클래스의 조건부 확률을 반환하도록 설계하여 멀티 클래스 문제를 해결할 수도 있다는 점이다.\n",
    "\n",
    "다음은 필기 숫자에 대한 영상 정보를 입력 받아 숫자 0 ~ 9 까지의 조건부 확률을 출력하는 MLP의 예이다. 입력 영상이 28 x 28 해상도를 가진다면 입력 계층의 뉴런 수는 $28 \\times 28 = 784$ 개가 된다. 출력은 숫자 0 ~ 9 까지의 조건부 확률을 출력하는 $10$ 개의 뉴런을 가진다.\n",
    "\n",
    "그림의 모형은 $15$개의 뉴런을 가지는 $1$ 개의 은닉 계층을 가진다.\n",
    "\n",
    "<img src=\"https://datascienceschool.net/upfiles/90f2752671424cef846839b89ddcf6aa.png\">\n",
    "\n",
    "\n",
    "## 신경망 가중치 표기법\n",
    "\n",
    "신경망의 가중치는 $w^{l}_{j,k}$ 과 같이 표기한다. 이 가중치는  $l-1$ 번째 계층의  $k$번째 뉴런와 $l$ 번째 계층의 $j$번째 뉴런을 연결하는 가중치를 뜻한다. 첨자의 순서에 주의한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
